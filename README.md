# CUDA-Accelerated K-Means Clustering

An optimized K-Means implementation leveraging NVIDIA GPUs. It features CUDA-accelerated **k-means++** initialization, **triangle-inequality** pruning (Elkan/Hamerly), **multi-stream** overlap of compute and transfer, and **batched** execution to scale beyond GPU memory. Data is stored in **Structure-of-Arrays (SoA)** layout for coalesced memory access, and **CSR graph representation** is used for sparse data handling.

<p align="center">
  <img src="https://img.shields.io/badge/CUDA-11%2B-76B900.svg"/> 
  <img src="https://img.shields.io/badge/CMake-3.20%2B-blue.svg"/> 
  <img src="https://img.shields.io/badge/License-MIT-lightgrey.svg"/>
</p>

---

## Why this project?

Classical K-Means is simple but memory-bound and distance-heavy. On modern GPUs, we can push it much further by:

- Parallelizing distance and centroid updates with custom CUDA kernels  
- Seeding with **k-means++** on GPU to reduce iterations  
- Cutting redundant distance evaluations with **Elkan/Hamerly** bounds  
- Overlapping hostâ†”device transfers with compute using **multiple CUDA streams**  
- **Batching** datasets that donâ€™t fit in device RAM (GPU memory).  
- Using **CSR graph representation** to efficiently handle sparse datasets

### Quantified results

- **Latency hiding:** Overlapping compute and memcpy hides ~**40%** of H2D/D2H time on PCIe.
- **Fewer distance computations:** Elkan/Hamerly pruning eliminates more than **50%** of distance evaluations after 2â€“3 iterations.

## Key Features

- âš¡ **GPU Acceleration:** CUDA kernels for distance calculations and centroid updates  
- ğŸ¯ **k-means++ Initialization:** GPU-parallelized seeding  
- ğŸ§® **Triangle-Inequality Pruning:** Elkan & Hamerly methods  
- ğŸ” **Multi-Stream Overlap:** multiple CUDA streams hide ~40% transfer latency  
- ğŸ“¦ **Batched Processing:** Works with datasets > GPU memory  
- ğŸ§± **SoA Layout:** Coalesced reads/writes for better memory throughput  
- ğŸ—œ **CSR Graph Representation:** Sparse datasets stored and processed efficiently
## Directory Structure

```
.
â”œâ”€â”€ Makefile                  # Builds the CUDA implementation (main.cu)
â”œâ”€â”€ main.cu                   # Main CUDA-accelerated K-Means implementation
â”œâ”€â”€ km.py                     # Scikit-learn KMeans implementation for baseline comparison
â”œâ”€â”€ checker.py                # Compares cluster assignments between CUDA and Scikit-learn outputs
â”œâ”€â”€ checker.sh                # Shell script to check closeness of CUDA vs Scikit-learn assignments
â”œâ”€â”€ script.sh                 # Runs both main.cu and km.py over datasets in input/ for comparison
â”œâ”€â”€ run.sh                    # Generates random test cases and tests them with main.cu & km.py
â”œâ”€â”€ TestGenerator.py           # Generates random datasets for testing
â”‚
â”œâ”€â”€ input/                    # Test dataset folder (dense text format)
â”‚   â”œâ”€â”€ dataset_1000.txt       # Dataset with 1000 points
â”‚   â”œâ”€â”€ dataset_10k.txt       # Dataset with 10K points
â”‚   â”œâ”€â”€ dataset_20k.txt       # Dataset with 20K points
â”‚
â”œâ”€â”€ cuML/                     # NVIDIA cuML-based implementation using cuBLAS
â”‚   â”œâ”€â”€ km_cu.py              # cuML KMeans implementation for performance comparison
â”‚   â”œâ”€â”€ input/                # Test dataset folder (same format as top-level input/)
â”‚     â”œâ”€â”€ dataset_1000.txt       # Dataset with 1000 points
â”‚     â”œâ”€â”€ dataset_10k.txt       # Dataset with 10K points
â”‚     â”œâ”€â”€ dataset_20k.txt       # Dataset with 20K points
â”‚   â””â”€â”€ script.sh             # Runs main.cu and km_cu.py for comparison on cuML
```
**Descriptions:**
- **Makefile** â€” Compilation instructions for CUDA code.  
- **main.cu** â€” Core CUDA implementation of K-Means.  
- **km.py** â€” Baseline K-Means using Scikit-learn for accuracy comparison.  
- **checker.py** â€” Reads CUDA and Scikit-learn outputs, compares assignments for similarity.  
- **checker.sh** â€” Automates comparison checks between CUDA and Scikit-learn outputs.  
- **script.sh** â€” Iterates through datasets in `input/` and runs both CUDA and Scikit-learn implementations.  
- **run.sh** â€” Generates random test cases (via `TestGenerator.py`) and evaluates both implementations.  
- **TestGenerator.py** â€” Creates synthetic datasets with specified dimensions, clusters, and points.  
- **input/** â€” Pre-generated test datasets for evaluation.  
- **cuML/** â€” Folder containing NVIDIA cuML (GPU library) K-Means implementation and its testing scripts.



## Requirements
- **CUDA** 11.4+ (tested on 11.8/12.x)
- **C++17** compiler (GCC 9+/Clang 12+)
- (Optional) Python 3.8+ for dataset generation scripts
  
## ğŸ“„ Input Format

The input file should be a **plain text file** with the following structure:

```
N d k
xâ‚â‚ xâ‚â‚‚ ... xâ‚d
xâ‚‚â‚ xâ‚‚â‚‚ ... xâ‚‚d
...
xNâ‚ xNâ‚‚ ... xNd
```


## Implementation Notes

- **SoA layout** ensures coalesced memory access  
- **CSR graph representation** is used to store sparse datasets, reducing memory footprint and speeding up distance computations by skipping zero entries  
- **k-means++** uses parallel prefix sums for better initial cluster selection  
- **Elkan/Hamerly bounds** reduce distance checks significantly  
- **Multi-stream pipelining** overlaps compute and transfers for 40% latency reduction

---

## Contributing

Pull requests welcome! Key focus areas:
  -  Optimize kernel memory access patterns
  -  Add sparse suppport
  -  Implement half-precision mode.


## Acknowledgements

- Elkan, C. (2003) "Using the Triangle Inequality to Accelerate k-Means"
- Hamerly, G. (2010) "Making k-Means Even Faster"
- NVIDIA Nsight for profiling
